name: etl-update

on:
  schedule:
    - cron: '0 3 * * FRI'
  workflow_dispatch:
    inputs:
      dry_run:
        description: 'Run as dry-run? true/false'
        required: false
        default: 'true'
      sample:
        description: 'Sample size for testing (0 = full)'
        required: false
        default: '10'

jobs:
  update:
    runs-on: ubuntu-latest
    timeout-minutes: 300

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Check secrets are set
        run: |
          [ -n "$AWS_ACCESS_KEY_ID" ] && echo "AWS_ACCESS_KEY_ID: set" || echo "AWS_ACCESS_KEY_ID: MISSING"
          [ -n "$AWS_SECRET_ACCESS_KEY" ] && echo "AWS_SECRET_ACCESS_KEY: set" || echo "AWS_SECRET_ACCESS_KEY: MISSING"
          [ -n "$S3_BUCKET_NAME" ] && echo "S3_BUCKET_NAME: set" || echo "S3_BUCKET_NAME: MISSING"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          S3_BUCKET_NAME: ${{ secrets.S3_BUCKET_NAME }}

      - name: Verify backup exists in S3
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          S3_BUCKET_NAME: ${{ secrets.S3_BUCKET_NAME }}
        run: |
          pip install awscli --quiet
          LATEST=$(aws s3 ls s3://$S3_BUCKET_NAME/duckdb_backup_ \
            --region ap-south-1 \
            | sort \
            | tail -n 1)
          if [ -z "$LATEST" ]; then
            echo "❌ No backup found in S3 — halting ETL for safety."
            exit 1
          fi
          echo "✓ Backup found: $LATEST"

          # Check backup is less than 2 days old
          BACKUP_DATE=$(echo "$LATEST" | awk '{print $1}')
          CUTOFF=$(date -d '2 days ago' +%Y-%m-%d)
          if [[ "$BACKUP_DATE" < "$CUTOFF" ]]; then
            echo "❌ Latest backup ($BACKUP_DATE) is older than 2 days — halting ETL."
            exit 1
          fi
          echo "✓ Backup is recent: $BACKUP_DATE"

      - name: Run ETL update
        env:
          TMDBAPIKEY: ${{ secrets.TMDBAPIKEY }}
          MOTHERDUCK_TOKEN: ${{ secrets.MOTHERDUCK_TOKEN }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          S3_BUCKET_NAME: ${{ secrets.S3_BUCKET_NAME }}
        run: |
          set -euo pipefail
          DRY="${{ github.event.inputs.dry_run || 'false' }}"
          SAMPLE="${{ github.event.inputs.sample || '0' }}"
          if [ "$DRY" = "true" ]; then
            echo "Running dry-run (sample=$SAMPLE)"
            if [ "$SAMPLE" != "0" ]; then
              python update_job.py --dry-run --sample="$SAMPLE"
            else
              python update_job.py --dry-run
            fi
          else
            echo "Running live update (sample=$SAMPLE)"
            if [ "$SAMPLE" != "0" ]; then
              python update_job.py --sample="$SAMPLE"
            else
              python update_job.py
            fi
          fi

      - name: Upload preview artifacts (dry-run only)
        if: ${{ github.event.inputs.dry_run == 'true' }}
        uses: actions/upload-artifact@v4
        with:
          name: tmdb-previews
          path: /tmp/update_job_*_preview_*.csv
          retention-days: 7
          if-no-files-found: warn